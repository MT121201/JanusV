## Janus Pro

## VSA

## V*

## Flash-Att
If this command raise an error
```
```
Please use up-to-date version from official (website)[https://github.com/Dao-AILab/flash-attention/releases/], note: has to be abiFALSE rather than abiTrue. Thanks to (Hansyvea)[https://github.com/Dao-AILab/flash-attention/issues/224#issuecomment-2084366991]
```
pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.2cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
``` 